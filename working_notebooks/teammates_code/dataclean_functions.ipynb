{"cells":[{"cell_type":"code","source":["from pyspark.sql import types, Window, functions as F\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pyspark.ml.feature import StringIndexer\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"49b5afdd-1216-430e-b498-de7e4633af80"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Airline Cleaning and Imputation\n1. `impute_null_numeric`: Takes in a dataset and a list of numeric features, then performs either mean or median imputation for the NULL values. The mean/median values are based on values from the same airport and month to impute more accurate values. \n2. `airline_field_cleaner`: Cleans up the weather fields for the master dataset. This drops errorneous and outlier values, performs 0 and median imputation for missing values, and finally converts all categorical features to numeric by either indexing or onehotencoding."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"175cb704-9157-488a-9e88-c417aa3826f5"}}},{"cell_type":"markdown","source":["## 1. impute_null_numeric"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bcd32e3-e958-4e8e-a79a-752927a1c4c8"}}},{"cell_type":"code","source":["def impute_null_numeric(df, vars_to_impute, origin_or_dest='origin', method='median'):\n    '''\n    Inputs:\n    df: full flights dataset\n    vars_to_impute: a list of variabe names to impute null values for. Only works for numeric variables for now\n    origin_or_dest: choose either origin or dest airport. Then this will be the airport used in the groupby\n    method: can be either median or mean\n    \n    Output:\n    pyspark dataframe with nulls replaced\n    '''\n    \n    if origin_or_dest == 'origin':\n        columns_to_groupby = ['year_airlns', 'month_airlns', 'origin_airlns']\n        \n    else:\n        columns_to_groupby = ['year_airlns', 'month_airlns', 'dest_airlns']\n    \n    for var in vars_to_impute:\n        \n        if method == 'mean':\n            grp_df = df.groupby(columns_to_groupby).agg(F.mean(var).alias('impute_val')).show(10)\n        \n        else:\n            grp_df = df.groupby(columns_to_groupby).agg(F.expr(f'percentile_approx({var}, .5)').alias('impute_val'))\n        \n        \n        df = df.join(grp_df, on=columns_to_groupby, how='left')\n        df = df.withColumn(var, F.when(F.col(var).isNull(), F.col('impute_val')).otherwise(F.col(var))).drop('impute_val')\n        \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"af493a21-258d-4529-aaf6-1941372636e1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2. airline_field_cleaner"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"165cce4f-eda5-414c-a3b2-da5c096ad514"}}},{"cell_type":"code","source":["def airline_field_cleaner(df, clean_null=True, remove_outliers=True, cat_var_encode_method='index'):\n    '''\n    Cleans every single airport column. Deals w/ NULLs and outliers.\n    Must pass in the final joined df because I use that column naming.\n    See google doc for reasoning behind these: https://docs.google.com/spreadsheets/d/1hj4W8B_U49jZOLH41LP1dL8OqRlD_-7m/edit#gid=764608588\n    '''\n    \n    if clean_null:\n        \n        # For some vars, impute values for nulls\n        # but these aren't used in the final dataset\n#         df = df.na.fill({\n#             'CANCELLATION_CODE_AIRLNS': 'missing',  # var is taken out of the final cleaned df\n#             'ARR_DELAY_NEW_AIRLNS': 0, \n#             'ARR_DEL15_AIRLNS': 0, \n#             'ARR_DELAY_AIRLNS': 0\n#             ,'ARR_DELAY_GROUP_AIRLNS': 0  # var is taken out of the final cleaned df\n#         })\n        \n        # drop some obs\n        # I previously also filtered out nulls for arr_time_airlns, but this was removed\n        df = df.filter(~F.col('DEP_DEL15_AIRLNS').isNull())\n        \n        # drop some columns\n        df = df.drop(*['CARRIER_DELAY_AIRLNS', 'WEATHER_DELAY_AIRLNS', 'NAS_DELAY_AIRLNS', 'SECURITY_DELAY_AIRLNS', 'LATE_AIRCRAFT_DELAY_AIRLNS']) \n        \n        # median imputation based on origin/dest flights\n        # I commented this out later b/c I guess we removed this var b/c we wouldn't know this ahead of time anyways. \n#         grp_df = df.groupby('origin_airlns', 'dest_airlns').agg(F.expr('percentile_approx(ACTUAL_ELAPSED_TIME_AIRLNS, .5)').alias('impute_val'))\n#         df = df.join(grp_df, on=['origin_airlns', 'dest_airlns'], how='left')\n#         df = df.withColumn('ACTUAL_ELAPSED_TIME_AIRLNS', \n#                            F.when(F.col('ACTUAL_ELAPSED_TIME_AIRLNS').isNull(), \n#                                   F.col('impute_val'))\\\n#                            .otherwise(F.col('ACTUAL_ELAPSED_TIME_AIRLNS')))\\\n#                 .drop('impute_val')\n        \n        # final na's to drop\n        df = df.filter(~F.col('CRS_ELAPSED_TIME_AIRLNS').isNull())\n        \n    \n    if remove_outliers:\n        \n        # removing extreme outliers\n        # removed: .filter(F.col('ACTUAL_ELAPSED_TIME_AIRLNS') != 1604)\\\n        df = df\\\n            .filter(F.col('CRS_ELAPSED_TIME_AIRLNS') != 813)\\\n            .filter(F.col('CRS_ELAPSED_TIME_AIRLNS') > 0)\\\n            .filter(F.col('ARR_DELAY_AIRLNS') <= 2300)\n        \n        \n    if cat_var_encode_method=='index':\n        \n        # Specify which columns to index (ie cast to int)\n        vars_to_index = [\n            'ORIGIN_AIRLNS', \n            'DEST_AIRLNS', \n            'OP_UNIQUE_CARRIER_AIRLNS' # a more granular form of origin/dest airlines\n        ]\n\n        # rename cols to drop them later\n        for var in vars_to_index:\n            df = df.withColumnRenamed(var, var+'_old')\n\n        # finally, index them\n        indexer = StringIndexer(inputCols=[i+'_old' for i in vars_to_index], outputCols=vars_to_index)\n        df = indexer.fit(df).transform(df)\n        df = df.drop(*[i+'_old' for i in vars_to_index])\n        \n        # drop cat vars that we won't be using\n        vars_to_drop = [\n            'ARR_TIME_BLK_AIRLNS', # already have the arrival time, so drop the blocks\n            'ORIGIN_CITY_NAME_AIRLNS', # too granular to model with\n            'DEST_CITY_NAME_AIRLNS',\n            'ORIGIN_STATE_ABR_AIRLNS', # already have state_fips which is numeric\n            'ORIGIN_STATE_NM_AIRLNS',\n            'DEST_STATE_NM_AIRLNS',\n            'DEST_STATE_ABR_AIRLNS', \n            'CANCELLATION_CODE_AIRLNS' # this is a response var, we won't know why a flight was cancelled in test data\n        ]\n        df = df.drop(*vars_to_drop)\n        \n        \n    if cat_var_encode_method=='onehotencode':\n        pass\n        # not built out yet b/c vini advised against this and our data has so many vars already.\n        # however, I think we'll need this if we use a logistic regression model. \n    \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad067742-5f98-4046-8b65-bbc3dd5e9dc6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["# Weather Setup and Preprocessing\nEstablishes dictionaries and lists to be used for weather cleaning and imputing."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"de8ce0b7-7ce4-4696-994c-913aa3a03019"}}},{"cell_type":"markdown","source":["## Generating dictionaries\nThis section generates dictionaries that are needed for use in functions. More context including data dictionary exploration linked [here](https://adb-731998097721284.4.azuredatabricks.net/?o=731998097721284#notebook/4070574709969671/command/4070574709969698)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7679cae0-ea63-4b50-a0ba-7eaec9a110cd"}}},{"cell_type":"code","source":["# data dict var names exact from documentation\n# I used the exact documentation name in order to make it easy to locate in my\n# spreadsheet here: https://docs.google.com/spreadsheets/d/1xthRtOjC5-kV0LMIRa_grkxZ6sLBlpqDpNwygOH_jlA/edit?usp=sharing\nvar_names_dict_base = {'WND': ['direction_angle', 'direction_quality_code', 'type_code', 'speed_rate', 'speed_quality_code'],\n                  'CIG': ['ceiling_height_dimension', 'ceiling_quality_code', 'ceiling_determination_code', 'CAVOK_code'],\n                  'VIS' : ['distance_dimension', 'distance_quality_code', 'variability_code', 'quality_variability_code'],\n                  'TMP' : ['air_temperature', 'air_temperature_quality_code'],\n                  'DEW' : ['dew_point_temperature', 'dew_point_quality_code'],\n                  'SLP' : ['sea_level_pressure', 'sea_level_pressure_quality_code'],\n                  'GA1' : ['coverage_code', 'coverage_quality_code', 'base_height_dimension', 'base_height_quality_code', 'cloud_type_code', 'cloud_type_quality_code'],\n                  'GF1' : ['total_coverage_code', 'total_opaque_coverage_code', 'quality_total_coverage_code', 'total_lowest_cloud_cover_code', 'quality_total_lowest_cloud_cover_code',\n                          'low_cloud_genus_code', 'quality_low_cloud_genus_code', 'lowest_cloud_base_height_dimension', 'lowest_cloud_base_height_quality_code', 'mid_cloud_genus_code',\n                          'quality_mid_cloud_genus_code', 'high_cloud_genus_code', 'quality_high_cloud_genus_code'],\n                  'MA1' : ['altimeter_setting_rate', 'altimeter_quality_code', 'station_pressure_rate', 'station_pressure_quality_code'],\n                  'REM' : ['remark_identifier', 'remark_length_quantity', 'remark_text'],\n                  'AA1' : ['period_quantity_in_hours', 'depth_dimension', 'condition_code', 'quality_code'],\n                  'AA2' : ['period_quantity_in_hours', 'depth_dimension', 'condition_code', 'quality_code'],\n                  'AJ1' : ['dimension', 'condition_code', 'quality_code', 'equivalent_water_depth_dimension', 'equivalent_water_condition_code', 'equivalent_water_condition_quality_code'],\n                  'AL1' : ['period_quantity', 'depth_dimension', 'condition_code', 'quality_code'],\n                  'AN1' : ['period_quantity', 'depth_dimension', 'condition_code', 'quality_code'],\n                  'AO1' : ['period_quantity_in_minutes', 'depth_dimension', 'condition_code', 'quality_code'],\n                  'AU1' : ['intensity_and_proximity_code', 'descriptor_code', 'precipitation_code', 'obscuration_code', 'other_weather_phenomena_code', 'combination_indicator_code', 'quality_code'],\n                  'AT1' : ['source_element', 'weather_type', 'weather_type_abbreviation', 'quality_code']\n                 }\n\n# dictionary with number of subfeatures for each weather variable\nnum_subfeatures_dict_base = {'WND' : 5,\n                        'CIG' : 4,\n                        'VIS' : 4,\n                        'TMP' : 2,\n                        'DEW' : 2,\n                        'SLP' : 2,\n                        'GA1' : 6,\n                        'GF1' : 13,\n                        'MA1' : 4,\n                        'REM' : 3,\n                        'AA1' : 4,\n                        'AA2' : 4,\n                        'AJ1' : 6,\n                        'AL1' : 4,\n                        'AN1' : 4,\n                        'AO1' : 4,\n                        'AU1' : 7,\n                        'AT1' : 4\n                       }\n\n# NAs are quality codes which don't indicate missing value\n# UNK means that it's not a quality code but no indicator available for missing data usually b/c categorical\nmissing_value_code_dict_base = {'WND' : [\"999\", \"9\",\"9\",\"9999\", \"NA\"],\n                          'CIG' : [\"99999\", \"NA\", \"9\", \"9\"],\n                          'VIS' : [\"999999\", \"NA\", \"9\", \"NA\"],\n                          'TMP' : [\"+9999\", \"NA\"],\n                          'DEW' : [\"+9999\", \"NA\"],\n                          'SLP' : [\"99999\", \"NA\"],\n                          'GA1' : [\"99\", \"NA\", \"+99999\", \"NA\", \"99\", \"NA\"],\n                          'GF1' : [\"99\", \"99\", \"NA\", \"99\", \"NA\", \"99\", \"NA\", \"99999\", \"NA\", \"99\", \"NA\", \"99\", \"NA\"],\n                          'MA1' : [\"99999\", \"NA\", \"99999\", \"NA\"],\n                          'REM' : [\"NA\", \"NA\", \"NA\"],\n                          'AA1' : [\"99\", \"9999\", \"9\", \"NA\"],\n                          'AA2' : [\"99\", \"9999\", \"9\", \"NA\"],\n                          'AJ1' : [\"9999\", \"9\", \"NA\", \"999999\", \"9\", \"NA\"],\n                          'AL1' : [\"99\", \"999\", \"9\", \"NA\"],\n                          'AN1' : [\"999\", \"9999\", \"9\", \"NA\"],\n                          'AO1' : [\"99\", \"9999\", \"9\", \"NA\"],\n                          'AU1' : [\"9\", \"9\", \"99\", \"9\", \"9\", \"9\", \"NA\"],\n                          'AT1' : [\"UNK\", \"UNK\", \"UNK\", \"NA\"]\n                          }\n\n# dictionary with type casting for each var\ntype_code_dict_base = {'WND' : ['float', 'int', 'str', 'float', 'int'],\n                  'CIG' : ['float', 'int', 'str', 'str'],\n                  'VIS' : ['float', 'int', 'int', 'int'],\n                  'TMP' : ['float', 'int'],\n                  'DEW' : ['float', 'int'],\n                  'SLP' : ['float', 'int'],\n                  'GA1' : ['str', 'int', 'float', 'int', 'str', 'int'],\n                  'GF1' : ['str', 'str', 'int', 'str', 'int', 'str', 'int', 'float', 'int', 'str', 'int', 'str', 'int'],\n                  'MA1' : ['float', 'int', 'float', 'int'],\n                  'AA1' : ['float', 'float', 'str', 'int'],\n                  'AA2' : ['float', 'float', 'str', 'int'],\n                  'AJ1' : ['float', 'str', 'int', 'float', 'int', 'int'],\n                  'AL1' : ['float', 'float', 'str', 'int'],\n                  'AN1' : ['float', 'float', 'str', 'int'],\n                  'AO1' : ['float', 'float', 'str', 'int'],\n                  'AU1' : ['str', 'str', 'str', 'str', 'str', 'str', 'int'],\n                  'AT1' : ['int', 'str', 'str', 'int']\n                 }\n\n# scaling factor for each var\n# 1.00 if int or not stated in documentation\nscale_factor_dict_base = {'WND' : [1.00, 1.00, 1.00, 10.00, 1.00],\n                     'CIG' : [1.00, 1.00, 1.00, 1.00],\n                     'VIS' : [1.00, 1.00, 1.00, 1.00],\n                     'TMP' : [10.00, 1.00],\n                     'DEW' : [10.00, 1.00],\n                     'SLP' : [10.00, 1.00],\n                     'GA1' : [1.00, 1.00, 1.00, 1.00, 1.00, 1.00],\n                     'GF1' : [1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00],\n                     'MA1' : [10.00, 1.00, 10.00, 1.00],\n                     'AA1' : [1.00, 10.00, 1.00, 1.00],\n                     'AA2' : [1.00, 10.00, 1.00, 1.00],\n                     'AJ1' : [1.00, 1.00, 1.00, 10.00, 1.00, 1.00],\n                     'AL1' : [1.00, 1.00, 1.00, 1.00],\n                     'AN1' : [1.00, 10.00, 1.00, 1.00],\n                     'AO1' : [1.00, 10.00, 1.00, 1.00],\n                     'AU1' : [1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00],\n                     'AT1' : [1.00, 1.00, 1.00, 1.00]\n                    }\n\n# list the columns to drop after splitting\n# Drops all quality codes as well as the following due to 100% null values:\n# VIS_WTHR_variability_code\n# AJ1_WTHR_equivalent_water_condition_code \n# GF1_WTHR_total_opaque_coverage_code\n# GF1_WTHR_low_cloud_genus_code\n# GF1_WTHR_mid_cloud_genus_code\n# GF1_WTHR_high_cloud_genus_code\n# AO1_WTHR_condition_code\n\nquality_code_dict_base = {'WND': ['direction_quality_code', 'speed_quality_code'],\n                  'CIG': ['ceiling_quality_code'],\n                  'VIS' : ['variability_code', 'distance_quality_code', 'quality_variability_code'],\n                  'TMP' : ['air_temperature_quality_code'],\n                  'DEW' : ['dew_point_quality_code'],\n                  'SLP' : ['sea_level_pressure_quality_code'],\n                  'GA1' : ['coverage_quality_code', 'base_height_quality_code', 'cloud_type_quality_code'],\n                  'GF1' : ['quality_total_coverage_code', \n                           'total_opaque_coverage_code', \n                           'quality_total_lowest_cloud_cover_code', \n                           'low_cloud_genus_code', 'quality_low_cloud_genus_code', \n                           'lowest_cloud_base_height_quality_code', \n                           'mid_cloud_genus_code', 'quality_mid_cloud_genus_code', \n                           'high_cloud_genus_code',  'quality_high_cloud_genus_code'], \n                  'MA1' : ['altimeter_quality_code', 'station_pressure_quality_code'],\n                  'REM' : ['remark_identifier', 'remark_length_quantity', 'remark_text'],\n                  'AA1' : ['quality_code'],\n                  'AA2' : ['quality_code'],\n                  'AJ1' : ['quality_code', 'equivalent_water_condition_code', 'equivalent_water_condition_quality_code'],\n                  'AL1' : ['quality_code'],\n                  'AN1' : ['quality_code'],\n                  'AO1' : ['condition_code', 'quality_code'],\n                  'AU1' : ['quality_code'],\n                  'AT1' : ['source_element', 'quality_code']\n                 }\n\n# generate a dict of vars to keep\nkept_vars_dict_base = {key: list(set(var_names_dict_base[key]) - set(quality_code_dict_base[key])) for key in var_names_dict_base}\n\n# dict of values to impute for numerical vars  \nnum_impute_dict = {'WND_WTHR_direction_angle': 1, 'WND_WTHR_direction_angle_origin': 1, 'WND_WTHR_direction_angle_dest': 1,\n               'WND_WTHR_speed_rate': 0, 'WND_WTHR_speed_rate_origin': 0, 'WND_WTHR_speed_rate_dest': 0,\n               'CIG_WTHR_ceiling_height_dimension': 0, 'CIG_WTHR_ceiling_height_dimension_origin': 0, 'CIG_WTHR_ceiling_height_dimension_dest': 0,\n               'VIS_WTHR_distance_dimension': 0, 'VIS_WTHR_distance_dimension_origin': 0, 'VIS_WTHR_distance_dimension_dest': 0,\n               'TMP_WTHR_air_temperature': 0, 'TMP_WTHR_air_temperature_origin': 0, 'TMP_WTHR_air_temperature_dest': 0,\n               'DEW_WTHR_dew_point_temperature': 0, 'DEW_WTHR_dew_point_temperature_origin': 0, 'DEW_WTHR_dew_point_temperature_dest': 0,\n               'SLP_WTHR_sea_level_pressure': 860, 'SLP_WTHR_sea_level_pressure_origin': 860, 'SLP_WTHR_sea_level_pressure_dest': 860,\n               'GA1_WTHR_base_height_dimension': 0, 'GA1_WTHR_base_height_dimension_origin': 0, 'GA1_WTHR_base_height_dimension_dest': 0,\n               'GF1_WTHR_lowest_cloud_base_height_dimension': 6000, 'GF1_WTHR_lowest_cloud_base_height_dimension_origin': 6000, 'GF1_WTHR_lowest_cloud_base_height_dimension_dest': 6000,\n               'MA1_WTHR_altimeter_setting_rate': 10132.5, 'MA1_WTHR_altimeter_setting_rate_origin': 10132.5, 'MA1_WTHR_altimeter_setting_rate_dest': 10132.5,\n               'MA1_WTHR_station_pressure_rate': 10132.5, 'MA1_WTHR_station_pressure_rate_origin': 10132.5, 'MA1_WTHR_station_pressure_rate_dest': 10132.5,\n               'AA1_WTHR_period_quantity_in_hours': 0, 'AA1_WTHR_period_quantity_in_hours_origin': 0, 'AA1_WTHR_period_quantity_in_hours_dest': 0,\n               'AA1_WTHR_depth_dimension': 0, 'AA1_WTHR_depth_dimension_origin': 0, 'AA1_WTHR_depth_dimension_dest': 0,\n               'AA2_WTHR_period_quantity_in_hours': 0, 'AA2_WTHR_period_quantity_in_hours_origin': 0, 'AA2_WTHR_period_quantity_in_hours_dest': 0,\n               'AA2_WTHR_depth_dimension': 0, 'AA2_WTHR_depth_dimension_origin': 0, 'AA2_WTHR_depth_dimension_dest': 0,\n               'AJ1_WTHR_dimension': 0, 'AJ1_WTHR_dimension_origin': 0, 'AJ1_WTHR_dimension_dest': 0,\n               'AJ1_WTHR_equivalent_water_depth_dimension': 0, 'AJ1_WTHR_equivalent_water_depth_dimension_origin': 0, 'AJ1_WTHR_equivalent_water_depth_dimension_dest': 0,\n               'AL1_WTHR_period_quantity': 0, 'AL1_WTHR_period_quantity_origin': 0, 'AL1_WTHR_period_quantity_dest': 0,\n               'AL1_WTHR_depth_dimension': 0, 'AL1_WTHR_depth_dimension_origin': 0, 'AL1_WTHR_depth_dimension_dest': 0,\n               'AN1_WTHR_period_quantity': 1, 'AN1_WTHR_period_quantity_origin': 1, 'AN1_WTHR_period_quantity_dest': 1,\n               'AN1_WTHR_depth_dimension': 0, 'AN1_WTHR_depth_dimension_origin': 0, 'AN1_WTHR_depth_dimension_dest': 0,\n               'AO1_WTHR_period_quantity_in_minutes': 0, 'AO1_WTHR_period_quantity_in_minutes_origin': 0, 'AO1_WTHR_period_quantity_in_minutes_dest': 0,\n               'AO1_WTHR_depth_dimension': 0, 'AO1_WTHR_depth_dimension_origin': 0, 'AO1_WTHR_depth_dimension_dest': 0}\n\n# list of values to impute for categorical vars  \ncat_impute_list = ['WND_WTHR_type_code', 'WND_WTHR_type_code_origin', 'WND_WTHR_type_code_dest',\n                   'CIG_WTHR_ceiling_determination_code', 'CIG_WTHR_ceiling_determination_code_origin', 'CIG_WTHR_ceiling_determination_code_dest',\n                   'CIG_WTHR_CAVOK_code', 'CIG_WTHR_CAVOK_code_origin', 'CIG_WTHR_CAVOK_code_dest', \n                   'AA1_WTHR_condition_code', 'AA1_WTHR_condition_code_origin', 'AA1_WTHR_condition_code_dest',\n                   'AA2_WTHR_condition_code', 'AA2_WTHR_condition_code_origin', 'AA2_WTHR_condition_code_dest',\n                   'AJ1_WTHR_condition_code', 'AJ1_WTHR_condition_code_origin', 'AJ1_WTHR_condition_code_dest',\n                   'AL1_WTHR_condition_code', 'AL1_WTHR_condition_code_origin', 'AL1_WTHR_condition_code_dest',\n                   'AN1_WTHR_condition_code', 'AN1_WTHR_condition_code_origin', 'AN1_WTHR_condition_code_dest',\n                   'AU1_WTHR_intensity_and_proximity_code', 'AU1_WTHR_intensity_and_proximity_code_origin', 'AU1_WTHR_intensity_and_proximity_code_dest',\n                   'AU1_WTHR_descriptor_code', 'AU1_WTHR_descriptor_code_origin', 'AU1_WTHR_descriptor_code_dest',\n                   'AU1_WTHR_precipitation_code', 'AU1_WTHR_precipitation_code_origin', 'AU1_WTHR_precipitation_code_dest',\n                   'AU1_WTHR_obscuration_code', 'AU1_WTHR_obscuration_code_origin', 'AU1_WTHR_obscuration_code_dest',\n                   'AU1_WTHR_other_weather_phenomena_code', 'AU1_WTHR_other_weather_phenomena_code_origin', 'AU1_WTHR_other_weather_phenomena_code_dest',\n                   'AU1_WTHR_combination_indicator_code', 'AU1_WTHR_combination_indicator_code_origin', 'AU1_WTHR_combination_indicator_code_dest',\n                   'AT1_WTHR_weather_type', 'AT1_WTHR_weather_type_origin', 'AT1_WTHR_weather_type_dest',\n                   'AT1_WTHR_weather_type_abbreviation', 'AT1_WTHR_weather_type_abbreviation_origin', 'AT1_WTHR_weather_type_abbreviation_dest',\n                    'GA1_WTHR_coverage_code', 'GA1_WTHR_coverage_code_origin', 'GA1_WTHR_coverage_code_dest',\n                    'GA1_WTHR_cloud_type_code', 'GA1_WTHR_cloud_type_code_origin', 'GA1_WTHR_cloud_type_code_dest',\n                    'GF1_WTHR_total_coverage_code', 'GF1_WTHR_total_coverage_code_origin', 'GF1_WTHR_total_coverage_code_dest',\n                    'GF1_WTHR_total_lowest_cloud_cover_code', 'GF1_WTHR_total_lowest_cloud_cover_code_origin', 'GF1_WTHR_total_lowest_cloud_cover_code_dest']"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a544f331-00ed-481a-b6f2-839c253c30d7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Weather Helper Functions and Preprocessing\n1. `append_WTHR`: fixes var naming in dicts to match our join naming (e.g. 'WND' --> 'WND_WTHR') for downstream things\n2. `append_feature_name`: preps some names for later when they're used to replace other names. appends the supervariable name to the subvariable so we will know where it came from (e.g. 'direction_angle' --> 'WND_WTHR_direction_angle')\n3. `list_weather_vars`: takes all elements of each (1) list within dict value and adds to a bigger list. used to generate lists of which weather subvariables to drop and which ones should be kept."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f0beffb-3ff2-4eec-a5ce-1de99f1da231"}}},{"cell_type":"code","source":["# function because I named dict variables wrong - appends \"_WTHR\" to the end\ndef append_WTHR(dictionary):\n    \"\"\"Appends \"_WTHR\" to the input dict key (e.g. 'WND' --> 'WND_WTHR')\n    Input: a dictionary\n    Output: a dictionary with keys renamed to str(key)+\"_WTHR\"\n    \"\"\"\n    weather_dict = dictionary\n    for old_key in list(weather_dict.keys()):\n        new_key = str(old_key) + '_WTHR'\n        weather_dict[new_key] = weather_dict.pop(old_key)\n    return weather_dict\n\n# fixes the names\n# dict base changes too \nvar_names_dict = append_WTHR(var_names_dict_base)\nnum_subfeatures_dict = append_WTHR(num_subfeatures_dict_base)\nmissing_value_code_dict = append_WTHR(missing_value_code_dict_base)\ntype_code_dict = append_WTHR(type_code_dict_base)\nscale_factor_dict = append_WTHR(scale_factor_dict_base)\nquality_code_dict = append_WTHR(quality_code_dict_base)\nkept_vars_dict = append_WTHR(kept_vars_dict_base)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"26d8e488-52f2-49d1-9898-87841a51bba6"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# function to append feature name in front of subfeature column names for var_names_dict\ndef append_feature_name(dictionary):\n    \"\"\" Appends the name of the key (Weather Dataset var name) in front of\n    each item in its respective list of values\n    e.g. 'direction_angle' --> 'WND_WTHR_direction_angle'\n  \n    Input: a dictionary where values are ideally a list of strings\n    Output: a dictionary with each element of values list (in this case the \n    list of subvariables) appended with the key in front\n    \"\"\"\n    input_dict = dictionary\n    var_names_dict = {}\n    for i in list(input_dict.keys()):\n        subfeature_list = []\n        for j in input_dict[str(i)]:\n            j = str(i) + \"_\" + str(j)\n            subfeature_list.append(j)\n        var_names_dict[i] = subfeature_list\n    return var_names_dict\n\n# append feature name to subfeature column names\nvar_names_dict = append_feature_name(var_names_dict)\nquality_code_dict = append_feature_name(quality_code_dict)\nkept_vars_dict = append_feature_name(kept_vars_dict)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82e6bf0e-00c0-438d-84b2-93805f15cd09"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# function to generate list of all potential weather var names\n\ndef list_weather_vars(dict):\n    \"\"\"Used for making a list of all weather variables names from a dict\n    (e.g. make a big list of the ones to drop)\n  \n    Input: dictionary where the format is key: value and value is a list of \n    strings (vs. a list of lists)\n    Output: a set of all of the values contained in the dict value lists \n    \"\"\"\n    dict = dict\n    list = []\n    for i in dict:\n        for j in dict[i]:\n            list.append(j)\n      \n    return list\n\n# generate list of all post-split weather variable quality codes (minus '_origin' and/or '_dest')\nnames_to_drop = set(list_weather_vars(quality_code_dict))\nkept_vars = set(list_weather_vars(kept_vars_dict))\nall_var_names = set(list_weather_vars(var_names_dict))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"27ff8a16-b2fe-4334-bcd5-9894ef17a6b3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Weather Cleaning and Imputation\n1. `generate_columns`: splits weather var columns (i.e. 'WND' is split into its 5 subvariables)\n2. `drop_qc_columns`: drops parent columns and quality code columns. also drops some categorical columns that contain 100% nulls\n3. `impute_weather_num`: imputes values (double type) for numerical weather variables\n4. `make_binary_cat_cols`: turns a categorical weather variable column into several binary columns (int type) depending on how many categories there are; drops the parent categorical column"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8533eb7b-b56f-4ee3-b411-1cf511298437"}}},{"cell_type":"markdown","source":["## 1. generate_columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f0359c88-b69f-45eb-9295-464ca0249c63"}}},{"cell_type":"code","source":["def generate_columns(df):\n    \"\"\"\n    Takes in a df, iterates through weather variable consisting of a few\n    subvariables (e.g. a 'WND' can look like '190,1,N,0015,1' and that's 5\n    subvars) and adds a column for each subvariable \n    \n    Input: dataframe\n    Output: dataframe with addition of weather subvariables as split columns (and dropping the supercolumns that were split)\n    \"\"\"\n  \n    df = df\n\n    for i in df.columns:\n        col_name = i\n        \n        try:\n            # save origin or destination flag if it exists\n            origin_dest = '_' + str(col_name).split('_')[-1].lower()\n\n            if origin_dest.lower() == '_dest' or origin_dest.lower() == '_origin':\n            # generate column name without origin and destination flag\n                split_location = len(origin_dest) * -1\n                temp_col_name = col_name[:split_location]\n            else:\n                temp_col_name = col_name\n\n            split_col = F.split(df[col_name], ',')\n        except IndexError:\n            continue\n    \n        try:\n\n            for j in range(0,num_subfeatures_dict[temp_col_name]):\n                if origin_dest.lower() == '_dest' or origin_dest.lower() == '_origin':\n                  # attach origin or destination flag back if it existed\n                    new_col_name = str(var_names_dict[temp_col_name][j]) + str(origin_dest)\n                else:\n                    new_col_name = str(var_names_dict[temp_col_name][j])\n\n                # make the new column and reassign\n\n                try:\n                    # generate new column & reassign based on variable type\n                    if type_code_dict[temp_col_name][j] == 'float':\n                        # casting 'floats' as doubles currently\n                        if int(scale_factor_dict[temp_col_name][j]) == 1.00:\n                            df = df.withColumn(new_col_name, F.when(split_col.getItem(j)==missing_value_code_dict[temp_col_name][j], F.lit(None)).otherwise(split_col.getItem(j).cast(types.DoubleType())))\n                        else:\n                            df = df.withColumn(new_col_name, F.when(split_col.getItem(j)==missing_value_code_dict[temp_col_name][j], F.lit(None)).otherwise(split_col.getItem(j).cast(types.DoubleType())/scale_factor_dict[temp_col_name][j]))                \n                    elif type_code_dict[temp_col_name][j] == 'str':\n                        df = df.withColumn(new_col_name,\n                                         F.when(split_col.getItem(j)==missing_value_code_dict[temp_col_name][j], F.lit(None)).otherwise(split_col.getItem(j)).cast(types.StringType()))\n                    elif type_code_dict[temp_col_name][j] == 'int':\n                        df = df.withColumn(new_col_name,\n                                 F.when(split_col.getItem(j)==missing_value_code_dict[temp_col_name][j], F.lit(None)).otherwise(split_col.getItem(j)).cast(types.IntegerType()))\n                except KeyError:\n                    continue\n        \n        except KeyError:\n              continue  \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b01b98c-c30c-40be-8609-de0095ff02cc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 2. drop_qc_columns\n\nThe following are all nulls, so dropped them in addition to dropping quality codes:<br>\nVIS_WTHR_variability_code <br>\nAJ1_WTHR_equivalent_water_condition_code <br>\nGF1_WTHR_total_opaque_coverage_code<br>\nGF1_WTHR_low_cloud_genus_code<br>\nGF1_WTHR_mid_cloud_genus_code<br>\nGF1_WTHR_high_cloud_genus_code"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"12886a0d-3dad-4ed7-a915-ebb8f84ce10d"}}},{"cell_type":"code","source":["def drop_qc_columns(df):\n    \"\"\" Drops columns that have to do with quality codes. Also drops some\n    catgorical weather variable columns we know to have 100% null values.\n  \n    Input: dataframe with weather variables already split\n    Output: dataframe with fewer columns\n  \n    \"\"\"\n    df = df\n\n    # iterate over all vars and slice the part before 2nd underscore\n    for i in df.columns:\n        try:\n            col_name = i\n            first_segment = i.split(\"_\")[0] + \"_\" + i.split(\"_\")[1]\n        except IndexError:\n            continue\n    \n        # if the slice matches a weather variable, then save origin/destination flag\n        # and split the name\n        if str(first_segment) in list(var_names_dict_base.keys()):\n            origin_dest = '_' + str(col_name).split('_')[-1].lower()\n      \n            # check for origin dest flag\n            if origin_dest.lower() == '_dest' or origin_dest.lower() == '_origin':\n                split_location = len(origin_dest) * -1\n                temp_col_name = col_name[:split_location]\n            else:\n                temp_col_name = col_name\n      \n            # drop vars not in the list of vars to keep\n            if temp_col_name not in list(kept_vars):\n                df = df.drop(col_name)\n            else:\n                continue\n      \n        else:\n            continue\n      \n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c86e0d37-65d5-4d13-a9d8-58842e5d121f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 3. impute_weather_num"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d798e3ee-f67e-45c3-8fca-cdcc63f11957"}}},{"cell_type":"code","source":["def impute_weather_num(df):\n    \"\"\"For certain numerical weather variables, imputes values where\n    there are nulls.\n  \n    Note: KeyError print statement doesn't mean something went wrong.\n    Variables throwing a KeyError are categorical and not numerical.\n  \n    Input: dataframe\n    Output: dataframe with numerical values imputed for numerical vars\n  \n    \"\"\"\n    df = df\n    temp_list_full_colname = []\n\n    # iterate over all vars and slice the part before 2nd underscore\n    for i in df.columns:\n        col_name = i\n        try:\n            first_segment = i.split(\"_\")[0] + \"_\" + i.split(\"_\")[1]\n        except IndexError:\n            continue\n    \n        # if the slice matches a weather variable, then save origin/destination flag\n        # and split the name\n        if str(first_segment) in list(kept_vars_dict_base.keys()):\n            origin_dest = '_' + str(col_name).split('_')[-1].lower()\n      \n            # check for origin dest flag\n            if origin_dest.lower() == '_dest' or origin_dest.lower() == '_origin':\n                split_location = len(origin_dest) * -1\n                temp_col_name = col_name[:split_location]\n            else:\n                temp_col_name = col_name\n      \n        # adds the column name to a list to be imputed\n        temp_list_full_colname.append(str(col_name))\n        num_impute_list = list(set(temp_list_full_colname))\n  \n    # imputes the correct value for each variable\n    enumerated_num_impute_list = enumerate(num_impute_list)\n    for pair in enumerated_num_impute_list:\n        try:\n            temp_dict = {pair[1]: num_impute_dict[pair[1]]}\n            df = df.na.fill(temp_dict)\n        except KeyError:\n            print(\"KeyError with {} (categorical var)\".format(pair[1]))\n\n    return df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6c4eebca-6a61-430b-8c24-cb389135310f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## 4. make_binary_cat_cols"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c4ccf765-f8a7-4e6c-a5cd-6613be85f27f"}}},{"cell_type":"code","source":["def make_binary_cat_cols(df):\n    \"\"\"Takes a dataframe and makes certain categorical weather variables into\n    binary variables (adds one column per category).\n  \n    Input: dataframe with columns containing categorical weather data\n    Output: dataframe with binary columns in place of the categorical ones\n  \n    \"\"\"\n    df = df\n    drop_list = []\n        \n    for i in df.columns:\n        if i in cat_impute_list:\n            col_name = i\n            origin_dest = '_' + str(col_name).split('_')[-1].lower()\n            drop_list.append(i)\n      \n            # get the general col name if there's an origin or dest tag at the end\n            if origin_dest.lower() == '_dest' or origin_dest.lower() == '_origin':\n                split_location = len(origin_dest) * -1\n                temp_col_name = col_name[:split_location]\n            else:\n                temp_col_name = col_name\n      \n            # depending on which column name it is make new binary columns based on possible\n            # categorical values. hardcoded based on which categorical codes show up per variable (see Categorical variables header in this notebook)\n            if temp_col_name == 'WND_WTHR_type_code':\n                # hardcode possible codes that are not nulls or empty\n                codes = ['V', 'C', 'N', 'R', 'H']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    # generate a new name for the binary column and cast 1 if it's the value and 0 if not\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'CIG_WTHR_ceiling_determination_code':\n                codes = ['M', 'C', 'W']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'CIG_WTHR_CAVOK_code':\n                codes = ['Y', 'N']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AA1_WTHR_condition_code':\n                codes = ['3', '1', '2']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AA2_WTHR_condition_code':\n                codes = ['M', 'C', 'W']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AJ1_WTHR_condition_code':\n                codes = ['3', '1']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AL1_WTHR_condition_code':\n                codes = ['3', '1']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AN1_WTHR_condition_code':\n                codes = ['3']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AU1_WTHR_intensity_and_proximity_code':\n                codes = ['0', '1', '2', '3', '4']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AU1_WTHR_descriptor_code':\n                codes = ['0', '1', '2', '3', '4', '5', '6', '7', '8']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AU1_WTHR_precipitation_code':\n                codes = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AU1_WTHR_obscuration_code':\n                codes = ['0', '1', '2', '3', '4', '5', '6', '7']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AU1_WTHR_other_weather_phenomena_code':\n                codes = ['0', '1', '2', '3', '4', '5']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AU1_WTHR_combination_indicator_code':\n                codes = ['1', '2', '3']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AT1_WTHR_weather_type':\n                codes = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10',\n                        '13', '14', '15', '16', '17', '18', '19', '21', '22']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'AT1_WTHR_weather_type_abbreviation':\n                codes = ['FG', 'BR', 'MIFG', 'FC', 'SN', 'DZ', 'RA', 'FZFG', 'TS', 'UP', 'FZDZ', 'BLSN', 'HZ', 'FZRA', 'GR', 'PL', 'DU', 'FG+']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'GA1_WTHR_coverage_code':\n                codes = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'GA1_WTHR_cloud_type_code':\n                codes = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '12', '15']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'GF1_WTHR_total_coverage_code':\n                codes = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n            elif temp_col_name == 'GF1_WTHR_total_lowest_cloud_cover_code':\n                codes = ['00', '01', '02', '03', '04', '05', '06', '07', '08', '09', '10']\n                for code in codes:\n                    print(\"currently on code {} in {}\".format(code, temp_col_name))\n                    new_col_name = col_name + '-' + code\n                    df = df.withColumn(new_col_name, F.when(df[col_name]==code, F.lit(1)).otherwise(F.lit(0)).cast(types.IntegerType()))\n                    print(\"-----generated column\", new_col_name)\n  \n        else:\n            continue\n  \n    df2 = df.drop(*drop_list)\n  \n    return df2    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c7ccab8-1180-4b74-8684-0073ec537428"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["d\n# Derive Flight Features\n1. `flight_derived_features_creation`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9cdf3287-6997-45d6-8078-e4c909313564"}}},{"cell_type":"markdown","source":["## 1. flight_derived_features_creation\n\nThis includes: \n\nFEATURE 1: **LOCAL_DEP_HOUR** (Extraction of Local Departure Hour) \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;0 - 23\n\nFEATURE 2: **HOLIDAY** (Holiday Indicator)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**0**: Not a holiday \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1**: Is a holiday \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**2**: Near holiday\n\n\nFEATURE 3: **Previous_Flight_Delay_15** (If the previous flight has delayed for 15 mins or more)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**0**: No Delay / Delay less than 15 minutes\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1**: Delay for 15 minutes or more\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**2**: Lack of information (The scheduled departure time of previous flight is less than two hours before the scheduled departure time of current flight)\n\n\nFEATURE 4: **Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep** (If there is enough time (40 minutes) between estimate arrival time of previous flight and planned departure time of current flight)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**0**: No Enough time \n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1**: Have Enough time\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**2**: Lack of information (The scheduled departure time of previous flight is less than two hours before the scheduled departure time of current flight)\n\n\nFEATURE 5: **Poor_Schedule** (If planned arrival time of Pre-Flight is later than planned departure time of Current-Flight)\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**0**: Not a poor scheduling\n\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**1**: Poor scheduling"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"763c6a21-6d55-4e78-a72a-5923f2609e27"}}},{"cell_type":"code","source":["def flight_derived_features_creation(df):\n    # FEATURE 1: Extraction of Local Departure Hour\n    df = df.withColumn(\"FORMATTED_CRS_DEP_TIME\", F.substring(F.format_string(\"0000%d\", \"CRS_DEP_TIME_AIRLNS\"), -4, 4))\n    df = df.withColumn(\"DATE_WITH_CRS_DEP_TIME\", F.concat_ws(\" \", df.FL_DATE_AIRLNS, df.FORMATTED_CRS_DEP_TIME))\n    df = df.withColumn(\"LOCAL_DEP_HOUR\", F.hour(F.to_timestamp(df.DATE_WITH_CRS_DEP_TIME, \"yyyy-MM-dd HHmm\")))\n    columns_to_drop = ['FORMATTED_CRS_DEP_TIME', 'DATE_WITH_CRS_DEP_TIME']\n    df = df.drop(*columns_to_drop)\n\n\n    # FEATURE 2: Create Holiday Indicator\n    df = df.withColumn(\"HOLIDAY\", F.expr(\"\"\"CASE WHEN FL_DATE_AIRLNS in (\n                              '2015-01-01', '2015-07-03', '2015-07-04', '2015-11-26', '2015-12-25',\n                              '2016-01-01', '2016-07-04', '2016-11-24', '2016-12-25', '2016-12-26', \n                              '2017-01-01', '2017-01-02', '2017-07-04', '2017-11-23', '2017-12-25',\n                              '2018-01-01', '2018-07-04', '2018-11-22', '2018-12-25', \n                              '2019-01-01', '2019-07-04', '2019-11-28', '2019-12-25') THEN 1 \"\"\" + \n         \"\"\" WHEN FL_DATE_AIRLNS in (\n                              '2015-01-02', '2015-01-03','2015-01-04', '2015-01-05', '2015-01-06', '2015-07-01', '2015-07-02', '2015-07-05', '2015-07-06', '2015-11-21', '2015-11-22', '2015-11-23', '2015-11-24', '2015-11-25', '2015-11-27', '2015-11-28', '2015-11-29', '2015-11-30', '2015-12-01', '2015-12-20', '2015-12-21', '2015-12-22', '2015-12-23', '2015-12-24', '2015-12-26', '2015-12-27','2015-12-28', '2015-12-29', '2015-12-30', '2015-12-31',\n\n                              '2016-01-02', '2016-01-03','2016-01-04', '2016-01-05', '2016-01-06', '2016-07-02', '2016-07-03', '2016-07-05', '2016-07-06', '2016-11-19', '2016-11-20', '2016-11-21', '2016-11-22', '2016-11-23', '2016-11-25', '2016-11-26', '2016-11-27', '2016-11-28', '2016-11-29', '2016-12-20', '2016-12-21', '2016-12-22', '2016-12-23', '2016-12-24', '2016-12-27', '2016-12-28','2016-12-29', '2016-12-30', '2016-12-31',\n\n                              '2017-01-03', '2017-01-04',  '2017-01-05', '2017-01-06', '2017-01-07', '2017-07-02', '2017-07-03', '2017-07-05', '2017-07-06','2017-11-18', '2017-11-19', '2017-11-20', '2017-11-21', '2017-11-22', '2017-11-24', '2017-11-25','2017-11-26', '2017-11-27', '2017-11-28', '2017-12-20', '2017-12-21', '2017-12-22', '2017-12-23', '2017-12-24', '2017-12-26', '2017-12-27', '2017-12-28', '2017-12-29', '2017-12-30', '2017-12-31', \n\n                              '2018-01-02', '2018-01-03', '2018-01-04', '2018-01-05', '2018-01-06', '2018-07-02', '2018-07-03', '2018-07-05', '2018-07-06','2018-11-17','2018-11-18', '2018-11-19', '2018-11-20', '2018-11-21', '2018-11-23', '2018-11-24','2018-11-25', '2018-11-26', '2018-11-27','2018-12-20','2018-12-21', '2018-12-22', '2018-12-23', '2018-12-24', '2018-12-26', '2018-12-27','2018-12-28', '2018-12-29','2018-12-30', '2018-12-31', \n\n                              '2019-01-02', '2019-01-03','2019-01-04', '2019-01-05', '2019-01-06', '2019-07-02', '2019-07-03', '2019-07-05', '2019-07-06', '2019-11-23', '2019-11-24', '2019-11-25', '2019-11-26', '2019-11-27', '2019-11-29', '2019-11-30','2019-12-01', '2019-12-02', '2019-12-03', '2019-12-20', '2019-12-21', '2019-12-22','2019-12-23', '2019-12-24', '2019-12-26', '2019-12-27','2019-12-28', '2019-12-29','2019-12-30', '2019-12-31') THEN 2 \"\"\"\n                              \"ELSE 0 END\"))\n\n    # FEATURE 3: Create Previous Flight Delay Indicator\n    # FEATURE 4: Calculate the estimate arrival time for the previous flight, \n    #            Get the time between previous flight estimate arrival time and CRS departure time for the current flight\n    # If time btwn estimate_arrival and crs_dep >= 40 minutes -------> 1, \n    #                                            < 40 minutes -------> 0, \n    #                          otherwise(lack of information) -------> 2   notes: The scheduled departure time of previous flight is less than two hours \n    #                                                                             before the scheduled departure time of current flight\n    # FEATURE 5: Create Poor_Schedule Indicator\n\n    utc_arrive_for_each_tail = Window.partitionBy('TAIL_NUM_AIRLNS').orderBy('utc_arrive')\n\n    # Two_Hour_Btwn_Prev_Departure_and_Current_Departure ----> \n    # 1: Time > 2 hours    0: Within 2 hours.(The scheduled departure time of previous flight is less than two hours \n    #                                         before the scheduled departure time of current flight)\n    utc_arrive_for_each_tail = Window.partitionBy('TAIL_NUM_AIRLNS').orderBy('utc_arrive')\n    df = df.withColumn('Prev_Flight_Planned_Departure_UTC', F.lag('utc_dep', 1).over(utc_arrive_for_each_tail))\\\n         .withColumn('Time_Btwn_Prev_Departure_and_Current_Departure', (F.unix_timestamp('utc_dep') - F.unix_timestamp('Prev_Flight_Planned_Departure_UTC')) /60 /60 ) \\\n         .withColumn('Two_Hour_Btwn_Prev_Departure_and_Current_Departure', F.expr(\"CASE WHEN Time_Btwn_Prev_Departure_and_Current_Departure > 2 THEN '1'\" + \"ELSE '0' END\"))\n\n    df = df.withColumn('Prev_Flight_Delay_15', F.lag('DEP_DEL15_AIRLNS', 1).over(utc_arrive_for_each_tail))\\\n         .withColumn('Prev_Flight_Delay', F.lag('DEP_DELAY_NEW_AIRLNS', 1).over(utc_arrive_for_each_tail))\\\n         .withColumn('Prev_Flight_Planned_Arrive_UTC', F.lag('utc_arrive', 1).over(utc_arrive_for_each_tail))\n\n    df = df.withColumn('Estimate_Pre_Flight_Arrival_Time', F.col(\"Prev_Flight_Planned_Arrive_UTC\") +  F.col(\"Prev_Flight_Delay\") * F.expr(\"Interval 1 Minutes\"))\n\n    # To get how many minutes between Estimate_Pre_Flight_Arrival_Time and utc_dep for the current flight\n    # And if there is enough time between Estimate_Pre_Flight_Arrival_Time and utc_dep for the current flight (40 minutes)\n    df = df.withColumn('Time_Btwn_Estimate_Arrival_and_Planned_Dep', (F.unix_timestamp('utc_dep') - F.unix_timestamp('Estimate_Pre_Flight_Arrival_Time')) / 60)\\\n         .withColumn('Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep', F.expr(\"CASE WHEN Time_Btwn_Estimate_Arrival_and_Planned_Dep >= 40 THEN 1 ELSE 0 END\"))\n\n    # Create Feature 'Poor_Schedule' if planned arrival time is later than planned departure time\n    df = df.withColumn('time_btwn_dep_and_plannedArrival', (F.unix_timestamp('utc_dep') - F.unix_timestamp('Prev_Flight_Planned_Arrive_UTC'))/60) \\\n         .withColumn('Poor_Schedule', F.expr(\"CASE WHEN time_btwn_dep_and_plannedArrival <= 0 THEN 1 ELSE 0 END\"))\n\n    # Update df:\n    # mark Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep = 2 for flights whose previous flight depart within 2 hours(Two_Hour_Btwn_Prev_Departure_and_Current_Departure == 0)\n    # mark Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep = 1 for flights that do not have previous flights\n    # mark Prev_Flight_Delay_15 = 0 for flights that do not have previous flights\n    # mark Poor_Schedule = 0 for flights that do not have previous flights\n\n\n    # Do have previous flight & previous flight depart less than 2 hours before the departure time of current flight, \n    # Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep = 2(lack of information, we don't know if there is enough time)\n    # Prev_Flight_Delay_15 = 2(lack of information, we don't know if the previous flight delay or not)\n    df = df.withColumn('Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep', F.when((df.Prev_Flight_Planned_Arrive_UTC.isNotNull()) & (df.Two_Hour_Btwn_Prev_Departure_and_Current_Departure == 0) , 2).otherwise(df.Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep))\n    df = df.withColumn('Prev_Flight_Delay_15', F.when((df.Prev_Flight_Planned_Arrive_UTC.isNotNull()) & (df.Two_Hour_Btwn_Prev_Departure_and_Current_Departure == 0) , 2).otherwise(df.Prev_Flight_Delay_15.cast(\"Integer\")))\n\n    # mark Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep = 1 for flights that do not have previous flights\n    # mark Prev_Flight_Delay_15 = 0 for flights that do not have previous flights\n    # mark Poor_Schedule = 0 for flights that do not have previous flights\n    df = df.withColumn('Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep', F.when(df.Prev_Flight_Planned_Arrive_UTC.isNull(), 1).otherwise(df.Enough_Time_Btwn_Estimate_Arrival_and_Planned_Dep))\\\n         .withColumn('Prev_Flight_Delay_15', F.when(df.Prev_Flight_Planned_Arrive_UTC.isNull(), 0).otherwise(df.Prev_Flight_Delay_15.cast(\"Integer\")))\\\n         .withColumn('Poor_Schedule', F.when(df.Prev_Flight_Planned_Arrive_UTC.isNull(), 0).otherwise(df.Poor_Schedule.cast(\"Integer\")))\n\n    # Drop no-longer needed columns\n    columns_to_drop = ['Time_Btwn_Estimate_Arrival_and_Planned_Dep', 'Estimate_Pre_Flight_Arrival_Time', 'Prev_Flight_Planned_Arrive_UTC', \n                     'Prev_Flight_Delay', 'Prev_Flight_Planned_Departure_UTC', 'Time_Btwn_Prev_Departure_and_Current_Departure', 'Two_Hour_Btwn_Prev_Departure_and_Current_Departure', 'time_btwn_dep_and_plannedArrival']\n    df = df.drop(*columns_to_drop)\n\n    return df\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55a4542b-ea4c-4388-94cb-84d4126860fd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["d\n# CV and Sampline Functions\n1. `make_cv_folds`: Take the entire data frame and split it into 5 folds of (train, set) dataframe tuples for rolling window based cross validation\n2. `undersample`: Take a dataframe of training data and undersamples the majority class to be about the same size as the minority class\n3. `oversample`: Take a dataframe of training data and oversamples (with replacement) the minority class to be about the same size as the majority class"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d4dd5f1-41df-49c9-ac7e-aa17d85c7d5b"}}},{"cell_type":"markdown","source":["## 1. make_cv_folds"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf87f70c-b10f-4eb2-9faf-1eb9f56f12a9"}}},{"cell_type":"code","source":["def make_cv_folds(data):\n    fold1_train = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2015-01-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2017-10-01'))\n    fold1_test = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2017-10-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-01-01'))\n\n    fold2_train = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2015-04-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-01-01'))\n    fold2_test = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2018-01-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-04-01'))\n\n    fold3_train = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2015-07-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-04-01'))\n    fold3_test = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2018-04-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-07-01'))\n\n    fold4_train = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2015-10-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-07-01'))\n    fold4_test = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2018-07-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-10-01'))\n\n    fold5_train = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2016-01-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2018-10-01'))\n    fold5_test = data.filter((F.to_timestamp(F.col('FL_DATE_AIRLNS')) >= '2018-10-01') & (F.to_timestamp(F.col('FL_DATE_AIRLNS')) < '2019-01-01'))\n\n    return [(fold1_train, fold1_test), (fold2_train, fold2_test), (fold3_train, fold3_test), (fold4_train, fold4_test), (fold5_train, fold5_test)]\n\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5c7698b-1331-49e8-ba1d-e37a6b9d511b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 2. undersample"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5be38500-7196-4b0c-b364-56af12540424"}}},{"cell_type":"code","source":["def undersample(data, label_col='DEP_DEL15_AIRLNS'):\n    delayed = data.filter(F.col(label_col) > 0)\n    not_delayed = data.filter(F.col(label_col) == 0)\n\n    delayed_count = delayed.count()\n    not_delayed_count = not_delayed.count()\n\n    sample_fraction = delayed_count * 1.0 / not_delayed_count\n\n    sample_not_delayed = not_delayed.sample(fraction=sample_fraction, seed=1)\n\n    return sample_not_delayed.union(delayed)\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"47917deb-d7e9-4c23-9dd4-f6d134848b68"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## 3. oversample"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e61a580f-37d6-4272-b7c7-5ecdf6a9e1ed"}}},{"cell_type":"code","source":["def oversample(data, over_sample_ratio=None, label_col='DEP_DEL15_AIRLNS'):\n    delayed = data.filter(F.col(label_col) > 0)\n    not_delayed = data.filter(F.col(label_col) == 0)\n\n    delayed_count = delayed.count()\n    not_delayed_count = not_delayed.count()\n\n    if not over_sample_ratio:\n        sample_count = not_delayed_count\n    else:\n        sample_count = delayed_count * over_sample_ratio\n    \n    not_delayed_sample_fraction = sample_count * 1.0 / not_delayed_count\n    delayed_sample_fraction = sample_count * 1.0 / delayed_count\n\n    sample_not_delayed = not_delayed.sample(fraction=not_delayed_sample_fraction, seed=1)\n    sample_delayed = delayed.sample(withReplacement=True, fraction=delayed_sample_fraction, seed=1)\n\n    return sample_delayed.union(sample_not_delayed)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d94e9944-9089-4aeb-9998-d83c6d47efb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"393c8fd5-b0c1-4970-a955-8d8c10f04645"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"dataclean_functions","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1898361324234161}},"nbformat":4,"nbformat_minor":0}
